{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "plt.style.use('fivethirtyeight')\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "img_dir = './Data'\n",
    "target_size = 64\n",
    "\n",
    "num_epochs = 50 # Number of training epochs at each model size\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "nz=100\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"Cuda is unavailable\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our data loader\n",
    "# https://pytorch.org/vision/stable/transforms.html\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, target_size):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = v2.Compose([\n",
    "            v2.Resize(size=(target_size, target_size)),\n",
    "            v2.ToDtype(torch.float32, scale=True)\n",
    "            # v2.ToTensor()\n",
    "        ])\n",
    "        self.imageLabels = os.listdir(img_dir)\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imageLabels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.imageLabels[idx])\n",
    "        image = torchvision.io.read_image(img_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        image = torch.from_numpy(np.rot90(image.T, 3).copy())\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsEAAAKcCAYAAAAJoWddAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVBElEQVR4nO3Ye6zfdX3H8Z5yOjosbXFdgqx0tB1sCBQRA8whyHVWFAxUKBeJOscmhktTHWgmynS4CFvIMISRMNydzWEQNhJljAFiOgZlMK4aWqA4Ki30QC+n9Hb238d8/+F3fktO3+eX1+Px9/uPV84vOb/n7zM0MjIyNgUAAIJMrR4AAAC7mwgGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAIM7weA/POOXkidzRl02btlZPaD525oerJ3R84Ut/XD2h2bp9c/WEno5b+p7qCc3ikyfPZ3fvPX9VPaFjwSGnV09obr7qguoJb+vW6/6wekKz6Ohjqyc0r//sleoJHU89s7J6QnP5l2+ontDTE489XT2h2XP6uNNpwj3/zH9VT+jYf+Fh1ROaww5f1PPGSzAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHGGx3v47z96eCJ39GX+vEOrJzR/8e3bqyd0bJs6Wj2hWb78a9UTepq5369WT2i2vPFC9YTmmBMvrJ7Q8fyqNdUTBsaCA3+tekLz+gEHVk9ofvLya9UTOl479qTqCQNl9qy9qyc023YNVU9oPn/FVdUTOv702m9WT2gOO3xRzxsvwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBke7+EXL7tiInf0Zfv2ydPuO/bYs3pCx12331s9oVm+/GvVE3qaf/gh1ROaRx77fvWE5sT3Xlo9oWPHmpXVEwbG+k2j1ROa7z3wg+oJzZGbd1RP6Fj15trqCT93QvWA3rZsnzyf30dPPb56QvPjVauqJ3Qsv2xZ9YRm8cfO6nkzeWoSAAB2ExEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAECc4fEebnpj2kTu6MvOXbuqJzQ7d75WPaFj9drqBYNl7Zpnqyc0Y6+/WD2h+cFt11dP6Pj0H/x59YSB8fRec6onNEMja6onNE/f/1D1hI7RDx5dPWGgfPKsD1VPaIanjlVPaE4/dXH1hI6rb/x29YS+eAkGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACDO0MjIyFj1CAAA2J28BAMAEEcEAwAQRwQDABBHBAMAEEcEAwAQRwQDABBHBAMAEEcEAwAQRwQDABBHBAMAEEcEAwAQRwQDABBHBAMAEEcEAwAQRwQDABBHBAMAEEcEAwAQRwQDABBHBAMAEEcEAwAQRwQDABBHBAMAEEcEAwAQRwQDABBHBAMAEEcEAwAQRwQDABBneLyHly2+dCJ39OXll16rntD8ZNPk2TJlypQpnz5mXvWE5vJ/vLl6Qk8zZ86sntCMje2sntC877i51RM6Nr+0T/WE5uH/WVE94W2tufOK6gnN2rUbqic0Ozeur57QceTxR1VPaKa978rqCT393rnnVE9oNm5YXT2hWb3qjeoJHUefuLh6QnP9Tdf3vPESDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAnOHxHq58/JGJ3NGXo87/ePWE5pm7/qF6QscvzPpQ9YSB8tEzj6+e0Cz57c9UT2jmLTi6ekLHmqkvVU8YGNu3jlZPaF54YW31hObgeTOqJ3S8tWlD9YRmWvWAcZi64fnqCc2NX/5K9YTmiVUvVk/oeOX5J6on9MVLMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcYbHezg6ddNE7ujL6jturp7QPPDQI9UTOpadcHb1hObiKZdXT+hp6Gcj1ROavfaZPL9Jlxz1keoJHVeuuKp6wsB48N6Hqyc026fuWT2hmXrAzOoJHUND1QsGyz7zjqye0AxN36t6QvPeU5dWT+i4469frJ7Ql8nzrQsAALuJCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAIM7weA8/fPCMidzRn20zqxc0t13yqeoJHVu2rK6eMFBWbdtZPaF5/L4fVU9oXlz3cvWEjpOPOLp6wsDYb87e1ROap9aMVE9o9hieXG8+u3aMVU8YKH/z3X+qntD8/b/cWT2huWLZ8uoJHd+44TvVE5oLvnBtz5vJ9V8BAAB2AxEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAHBEMAEAcEQwAQBwRDABAnKGRkZGx6hEAALA7eQkGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgzvB4D+++5aaJ3NGXZV+/tnrCpHX1Zb9TPaFZevmXqif09OA/3109ofn4A3OrJzQbb/nN6gmT1ubNm6snvK15C/evntAMvbWxekJz4ZknV0/o+P7KVdUTmod/uLJ6Qk/vOv9b1ROaq5ccWT2h+cYt/1Y9oWNo3sLqCc2qG8/reeMlGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOMPjPZwxe/YEzujT0I7qBc2M6TOqJ3Qcfui7qycMlJu+dUP1hObEPSbTb9L/qB7A/9OFl15SPaGZs8++1ROa9x92WPWEjvectrZ6wkD5lb1nVU9oTj/2kOoJzTGHHlQ9oeOkP7qtekJfJtO3LgAA7BYiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDjD4z38pTnvnMgdfdl3ztzqCc2WLW9WT+j45dn7VE8YKNNnvKN6QrN58+bqCc1Fpy2pntCxbsP66gnN397zr9UT3tYXL1lePaF55oHJ87fad7951RM6FiycXz1hoIzuv6h6QnPn3Q9VT2jm7Du7ekLH7N94f/WEvngJBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACCOCAYAII4IBgAgjggGACDO8HgPD5q730Tu6Mtf3vDV6gnNGZ+4rHpCx+joaPWEgbJ169bqCZPS65vfrJ7Qccstt1ZPGBjTx6oX/NwRHzitesKktei3rqye0Dy14trqCT2te/K+6gnNNf97YPWEZtr8fasndOwcd1VODl6CAQCII4IBAIgjggEAiCOCAQCII4IBAIgjggEAiCOCAQCII4IBAIgjggEAiCOCAQCII4IBAIgjggEAiCOCAQCII4IBAIgjggEAiCOCAQCII4IBAIgjggEAiCOCAQCII4IBAIgjggEAiCOCAQCII4IBAIgjggEAiCOCAQCII4IBAIgjggEAiCOCAQCII4IBAIgjggEAiCOCAQCII4IBAIgjggEAiCOCAQCIMzQyMjJWPQIAAHYnL8EAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEGR7v4WcuWDqRO/rygVMWV09onnvuueoJHdu2ba2e0Fx33Z9VT+jprvvuqZ7Q7Nr5VvWEZuPGjdUTOn76ytrqCc0VFy+rnvC25syZUz2heXTlyuoJzfp166sndDw7ib47zj/v3OoJPd3xn/dVT2heffah6gnN8ku/WT2h4/c/e0r1hObaP7m9542XYAAA4ohgAADiiGAAAOKIYAAA4ohgAADiiGAAAOKIYAAA4ohgAADiiGAAAOKIYAAA4ohgAADiiGAAAOKIYAAA4ohgAADiiGAAAOKIYAAA4ohgAADiiGAAAOKIYAAA4ohgAADiiGAAAOKIYAAA4ohgAADiiGAAAOKIYAAA4ohgAADiiGAAAOKIYAAA4ohgAADiiGAAAOKIYAAA4ohgAADiiGAAAOKIYAAA4gyP9/CgQ4+YyB19Wb36yeoJzWOPTZ4tU6ZMmXLBJz9VPWGgTJ06eX4HXvP5r1RPaEZH36qe0LH04ouqJwyM+++/v3pC89kLz6qe0Ly2dY/qCR2L5s+qntCcf9651RN6evye71RPaPb8xYOqJzTLPre0ekLHrFkzqif0ZfIUAAAA7CYiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDjD4z386jVfn8gdffndCz5RPaGZvue06gkdf3frrdUTmvPPObt6Qk+LFsytntCcedYZ1ROa797+veoJHcf8+sLqCQPj0UdXVE9olpy9pHpCs+K/n62e0LH+1XXVEwbKAQsPr57Q7HfAouoJzZy9jq+e0PHTdT+untAXL8EAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBHBAADEEcEAAMQRwQAAxBke7+GpJ3xw4lb0aWjnjuoJzR7T31E9oWNo2O+afszY+53VE5qLP3dR9YTm4HcvqJ7QsWHk1eoJA+PZ51ZVT2j23mta9YTmhw+uqJ7QccJJx1VPGChz37WwekKza/uG6gnNCy9vqZ7Q8cyT91ZPaD5y8jk9bxQTAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxRDAAAHFEMAAAcUQwAABxhkZGRsaqRwAAwO7kJRgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4IhgAgDgiGACAOCIYAIA4/weoZLtpMbXOaAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the image loader\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "training_data = ImageDataset(img_dir, 4)\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 4, 4\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_vector(shape: torch.Size):\n",
    "    \"\"\"\n",
    "    Generate a random image with channels, width, and height defined by shape\n",
    "    This is useful for testing that the discriminator can receive the correct inputs and outputs\n",
    "    \"\"\"\n",
    "    channels = shape[0]\n",
    "    width = shape[1]\n",
    "    height = shape[2]\n",
    "    return np.random.rand(channels, width, height)\n",
    "# img = generate_random_vector(training_data[0].shape)\n",
    "# plt.axis('off')\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Used to linearly combine two layers\n",
    "    The first layer is multiplied by alpha, while the second is multiplied by 1-alpha\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, alpha_decay=0.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "    def forward(self, x1, x2):\n",
    "        self.alpha -= self.alpha_decay\n",
    "        self.alpha = max(0, self.alpha)\n",
    "        return self.alpha * x1 + (1- self.alpha) * x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the weighted layer is working correctly\n",
    "\n",
    "# Test using two numbers\n",
    "layer = WeightedLayer(alpha=0.9) # Take 90% of x and 10% of x2\n",
    "assert(layer(10, 1) == 9.1)\n",
    "\n",
    "# Test using two arrays\n",
    "a = np.ones(shape=(2, 2))\n",
    "b = np.zeros(shape=(2, 2))\n",
    "assert(np.all(layer(a,b) == 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, add weightedlayer\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # self.resolution = 3*4*4\n",
    "        self.resolution = 4\n",
    "        self.layers = []\n",
    "        self.nn = torch.nn.Sequential()\n",
    "        self.nnNew = torch.nn.Sequential()\n",
    "        self.weighted = None\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # if (len(x.shape) > 1):\n",
    "        #     x = x.view(-1)\n",
    "        \n",
    "        if self.weighted is not None:\n",
    "            val = self.weighted(self.nn(x), self.nnNew(x))\n",
    "            if (len(val.shape) > 1):\n",
    "                print(\"ERROR on discriminator output size after weighted layer\")\n",
    "                print(f\"Val is {val}\")\n",
    "            return val\n",
    "        val = self.nn(x)\n",
    "        if (len(val.shape) > 1):\n",
    "            print(\"ERROR on discriminator output size\")\n",
    "            print(f\"Val is {val}\")\n",
    "        return val\n",
    "    \n",
    "    def double_size(self):\n",
    "        \"\"\"\n",
    "        !Must only be called after fully training the last layers!\n",
    "        Adds an additional bundle layer to end of discriminator, doubling possible image size\n",
    "        \"\"\"\n",
    "        if self.weighted is not None:\n",
    "            self.nn = self.nnNew\n",
    "        self.add_layer_bundle()\n",
    "        self.resolution *= 2\n",
    "        self.weighted = WeightedLayer(alpha=0.995, alpha_decay=0.001)\n",
    "        \n",
    "    def add_layer_bundle(self):\n",
    "        \"\"\"\n",
    "        A layer bundle consists of a WeightedLayer, a Conv2D, a BatchNorm, and a ReLU\n",
    "        each is added to the end of layers\n",
    "        \"\"\" \n",
    "        in_channels = self.resolution\n",
    "        out_channels = self.resolution * 2\n",
    "        ConvLayer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=2)\n",
    "        BatchNormLayer = torch.nn.BatchNorm2d(out_channels)\n",
    "        ReLULayer = torch.nn.ReLU()\n",
    "\n",
    "        self.layers.append(ConvLayer)\n",
    "        self.layers.append(BatchNormLayer)\n",
    "        self.layers.append(ReLULayer)\n",
    "        \n",
    "    def finalize_model(self):\n",
    "        if self.weighted is not None:\n",
    "            self.nnNew = torch.nn.Sequential()\n",
    "            for count, layer in enumerate(self.layers):\n",
    "                self.nnNew.add_module(str(count), layer)\n",
    "            \n",
    "            self.nnNew.add_module(\"FinalConv\", torch.nn.Conv2d(self.resolution, 1, 1, 4, 1, 0))\n",
    "            self.nnNew.add_module(\"Sigmoid\", torch.nn.Sigmoid())\n",
    "            self.nnNew = self.nnNew.to(device)\n",
    "        else:\n",
    "            self.nn = torch.nn.Sequential()\n",
    "            for count, layer in enumerate(self.layers):\n",
    "                self.nn.add_module(str(count), layer)\n",
    "            \n",
    "            self.nn.add_module(\"FinalConv\", torch.nn.Conv2d(self.resolution, 1, kernel_size=2))\n",
    "            self.nn.add_module(\"Sigmoid\", torch.nn.Sigmoid())\n",
    "            self.nn = self.nn.to(device)\n",
    "    def display_model(self):\n",
    "        print(f\"Resolution {self.resolution}\")\n",
    "        for layer in self.nn:\n",
    "            print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution 512\n",
      "Conv2d(4, 8, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Conv2d(256, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      "Sigmoid()\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "discriminator.double_size()\n",
    "discriminator.double_size()\n",
    "discriminator.double_size()\n",
    "discriminator.double_size()\n",
    "discriminator.double_size()\n",
    "discriminator.double_size()\n",
    "discriminator.finalize_model()\n",
    "discriminator.double_size()\n",
    "discriminator.finalize_model()\n",
    "discriminator.display_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add weighted layer\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.resolution = 6\n",
    "        self.layers = [\n",
    "            torch.nn.ConvTranspose2d(6, 3, 4, 2, 1, bias=False),\n",
    "            torch.nn.Tanh()\n",
    "        ]\n",
    "        self.weighted = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.weighted is not None:\n",
    "            return self.weighted(self.nn(x), self.nnNew(x))\n",
    "        return self.nn(x)\n",
    "    \n",
    "    def double_size(self):\n",
    "        \"\"\"\n",
    "        !Must only be called after fully training the last layers!\n",
    "        Adds an additional bundle layer to end of generator, doubling output size\n",
    "        \"\"\"\n",
    "        if self.weighted is not None:\n",
    "            self.nn = self.nnNew\n",
    "        self.add_layer_bundle()\n",
    "        self.resolution *= 2\n",
    "        self.weighted = WeightedLayer(alpha = 0.995, alpha_decay = 0.001)\n",
    "\n",
    "    def add_layer_bundle(self):\n",
    "        \"\"\"\n",
    "        A layer bundle consists of a ConvTranspose2D, a BatchNorm, and a ReLU\n",
    "        each is added to the beginning of layers\n",
    "        \"\"\"\n",
    "        in_channels = self.resolution * 2\n",
    "        out_channels = self.resolution\n",
    "        ConvLayer = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        BatchNormLayer = torch.nn.BatchNorm2d(out_channels)\n",
    "        ReLULayer = torch.nn.ReLU()\n",
    "        \n",
    "        self.layers = [ConvLayer, BatchNormLayer, ReLULayer] + self.layers\n",
    "        \n",
    "    def finalize_model(self):\n",
    "        self.initialLayer = torch.nn.ConvTranspose2d(nz, self.resolution, kernel_size=2, stride=1, padding=0)\n",
    "        if self.weighted is not None:\n",
    "            self.nnNew = torch.nn.Sequential(self.initialLayer)\n",
    "            for count, layer in enumerate(self.layers):\n",
    "                self.nnNew.add_module(str(count+1), layer)\n",
    "            self.nnNew = self.nnNew.to(device)\n",
    "        else:\n",
    "            self.nn = torch.nn.Sequential(self.initialLayer)\n",
    "            for count, layer in enumerate(self.layers):\n",
    "                self.nn.add_module(str(count+1), layer)\n",
    "            self.nn = self.nn.to(device)\n",
    "        \n",
    "    def display_model(self):\n",
    "        print(f\"Resolution {self.resolution}\")\n",
    "        for layer in self.nn:\n",
    "            print(layer)\n",
    "        if self.nnNew is not None:\n",
    "            print(\"\\n\\nNew Model:\")\n",
    "            for layer in self.nnNew:\n",
    "                print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution 768\n",
      "ConvTranspose2d(100, 384, kernel_size=(2, 2), stride=(1, 1))\n",
      "ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(96, 48, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(48, 24, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(24, 12, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(12, 6, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(6, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Tanh()\n",
      "\n",
      "\n",
      "New Model:\n",
      "ConvTranspose2d(100, 768, kernel_size=(2, 2), stride=(1, 1))\n",
      "ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(96, 48, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(48, 24, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(24, 12, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(12, 6, kernel_size=(2, 2), stride=(1, 1))\n",
      "BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ConvTranspose2d(6, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Tanh()\n"
     ]
    }
   ],
   "source": [
    "generator = Generator().to(device)\n",
    "generator.double_size()\n",
    "generator.finalize_model()\n",
    "generator.double_size()\n",
    "generator.finalize_model()\n",
    "generator.double_size()\n",
    "generator.finalize_model()\n",
    "generator.double_size()\n",
    "generator.finalize_model()\n",
    "generator.double_size()\n",
    "generator.finalize_model()\n",
    "generator.double_size()\n",
    "generator.finalize_model()\n",
    "generator.double_size()\n",
    "generator.finalize_model()\n",
    "generator.display_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights for generator and discriminator\n",
    "# From https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(netG, netD, dataloader, num_epochs, device, criterion, optimizerD, optimizerG):   \n",
    "    nz = 100\n",
    "    real_label = 1.\n",
    "    fake_label = 0\n",
    "    # Lists to keep track of progress\n",
    "    img_list = []\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    iters = 0\n",
    "    \n",
    "    print(\"Starting Training Loop...\")\n",
    "    # For each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # For each batch in the dataloader\n",
    "        for i, data in enumerate(dataloader):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            ## Train with all-real batch\n",
    "            netD.zero_grad()\n",
    "            # Format batch\n",
    "            real_cpu = data[0].to(device)\n",
    "\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "            \n",
    "            # Forward pass real batch through D\n",
    "            output = netD(real_cpu).view(-1)\n",
    "            print(output)\n",
    "            \n",
    "            \n",
    "            # Calculate loss on all-real batch\n",
    "            errD_real = criterion(output, label)\n",
    "            # Calculate gradients for D in backward pass\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "    \n",
    "            ## Train with all-fake batch\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            # Generate fake image batch with G\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            # Classify all fake batch with D\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            errD_fake = criterion(output, label)\n",
    "            # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            # Compute error of D as sum over the fake and the real batches\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "    \n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            output = netD(fake).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = criterion(output, label)\n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "    \n",
    "            # Output training stats\n",
    "            if i % 50 == 0:\n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                      % (epoch, num_epochs, i, len(dataloader),\n",
    "                         errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "    \n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(errG.item())\n",
    "            D_losses.append(errD.item())\n",
    "    \n",
    "            # Check how the generator is doing by saving G's output on fixed_noise\n",
    "            if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "                with torch.no_grad():\n",
    "                    fake = netG(fixed_noise).detach().cpu()\n",
    "                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "    \n",
    "            iters += 1\n",
    "    return (netG, netD, img_list)\n",
    "def evaluate(generator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (nn): Sequential(\n",
      "    (FinalConv): Conv2d(4, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (Sigmoid): Sigmoid()\n",
      "  )\n",
      "  (nnNew): Sequential()\n",
      ")\n",
      "Starting Training Loop...\n",
      "ERROR on discriminator output size\n",
      "Val is tensor([[[0.5450, 0.5425],\n",
      "         [0.5644, 0.5647],\n",
      "         [0.4977, 0.5031]]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.5450, 0.5425, 0.5644, 0.5647, 0.4977, 0.5031], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([4])) that is different to the input size (torch.Size([6])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m resolution \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(discriminator)\n\u001b[0;32m---> 25\u001b[0m generator, discriminator, img_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m evaluate(generator)\n\u001b[1;32m     27\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mdouble_size()\n",
      "Cell \u001b[0;32mIn[115], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(netG, netD, dataloader, num_epochs, device, criterion, optimizerD, optimizerG)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate loss on all-real batch\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m errD_real \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Calculate gradients for D in backward pass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m errD_real\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/pipx/venvs/notebook/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/pipx/venvs/notebook/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/pipx/venvs/notebook/lib/python3.11/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/pipx/venvs/notebook/lib/python3.11/site-packages/torch/nn/functional.py:3113\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3111\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3116\u001b[0m     )\n\u001b[1;32m   3118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3119\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([6])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)\n",
    "\n",
    "discriminator.apply(weights_init)\n",
    "generator.apply(weights_init)\n",
    "\n",
    "discriminator.finalize_model()\n",
    "generator.finalize_model()\n",
    "\n",
    "\n",
    "\n",
    "loss = torch.nn.BCELoss()\n",
    "\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "\n",
    "resolution = 4\n",
    "for resolution in [4, 8, 16, 32, 64, 128, 256]:\n",
    "    # dataloader = ImageDataset(img_dir, resolution)\n",
    "    training_data = ImageDataset(img_dir, resolution)\n",
    "    # dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    dataloader = torch.utils.data.DataLoader(training_data, batch_size=16)\n",
    "    resolution *= 2\n",
    "    print(discriminator)\n",
    "    generator, discriminator, img_list = train(generator, discriminator, dataloader, num_epochs, device, loss, discriminator_optimizer, generator_optimizer)\n",
    "    evaluate(generator)\n",
    "    discriminator.double_size()\n",
    "    discriminator.finalize_model()\n",
    "    generator.double_size()\n",
    "    generator.finalize_model()\n",
    "print(discriminator)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
